# Question Answering over Freebase with Multi-Column Convolutional Neural Networks

## 摘要

利用知识库来回答自然语言问题是一个很重要也很有挑战性的任务。很多现有的系统以来书写规则来帮助理解问题和答案排名。本文中介绍了了多列卷积神经网络(MCCNN)来从三个不同的方面理解问题（即，答案路径，答案上下文环境，和答案类型），并且学习了他们的分布式表示。同时，我们学习了知识库中的实体和关系的低维表示。问答对训练出的模型用来对候选答案排序。我们同样适用问题的释义来使用多任务学习方法训练列网络。我们使用FreeBase作为知识库，并用WEBQUESTIONs数据集做了扩展实验。实验结果表明，我们的方法相比baseline系统获得了更好的性能。另外，我们研发了在不同列网络中计算问题单词的突出分数的方法。这个结果帮助我们直观的理解MCCNN的学习。

## 1 简介

自动问答系统可以返回自然语言问题的确切的答案。近些年来，大规模知识库，例如Freebase,提供了丰富的资源来回答开放领域的问题。然而，如何理解自然语言问题，以及架设从自然语言到结构化的知识库的桥梁，仍然是一个具有挑战性的问题。

到目前为止，问题任务主要有两种方法。第一种是基于语义解析的（Berant et al.,Berant and Liang,2014）另一种依赖结构化知识库上的信息抽取（Yao and Van Durme,2014;Bordes et al.,2014a;Bordes et al,2014b）.语义解析通过把自然语言问题转换成逻辑形式来理解自然语言问题。然后，用解析结果来生成结构化的查询语句，从知识库中搜索并获得答案。最近的工作主要焦点是利用问答对，而不是标注的逻辑形式的问题，作为弱训练信号（Liang et al,2011;Krishnaurthy and Mitchell,2012）,从而减少标注成本。然而，这些仍然需要一个固定的和预定义的词汇触发器集，这就限制了他们的适用领域和扩展能力。另外，他们需要手动的设计语义解析的特征。第二种方法使用开放问答的信息提取技术。这些方法从知识库里检索出候选答案，然后提取问题和候选答案的特征来对他们进行排序。然而，这个方法（Yao an VanDurme,2014）依赖于规则和依存解析结果来提取问题的书写特征。另外，一些方法（Borders et al,2014a;Bordes et al,2014b）使用问题词嵌入的和来表示问题，这种方法忽略了词的顺序信息，不能处理复杂的问题。

本文介绍了multi-column卷积神经网络(MCCNN）来自动从不同的方面分析问题。特别的，这个模型共享相同的词嵌入来表示问题中的词。MCCNNs使用不同列的网络来从问题中抽取答案类型，关系和上下文信息。知识库中的实体和关系（本文中是Freebase）也用低维的向量表示。然后，用一个评分层根据问题和候选答案的表示来排序。
本文提出的基于信息抽取的方法利用问答对自动的学习模型，而不需要手动的标注逻辑形式和书写特征。我们也没有使用任何预定义的词汇触发器和规则。另外，问题释义（question paraphrase）也用来训练网络和归纳多任务学习方法中未出现的词。

本文主要研究了三个方面：
1.介绍了使用多列卷积神经网络来理解问题而不是依赖书写特征和规则，并使用问题释义来训练列网络和词向量。
2.接着学习了FreeBase中实体和关系的低维嵌入，使用问答对作为监督信号。
3.在WEBQuestion数据集上做了扩展实验，并提供了MCCNN的直观的解释。

## 2 相关工作

另一方面，基于信息检索的系统检索到候选集然后进行深入的分析来获得答案。他们的主要不同是如何从候选集中找到正确的答案。Yao 和Van Durme（2014）使用规则来抽取问题特征，并使用检索到的主题图的关系和属性作为知识库的特征。然后，这两种特征输入到logistic回归模型中来把问题的候选答案分类为correct/wrong。相反的，本文不使用规则，依存解析结果或书写特征来理解问题。另外一些工作（Bordes etal,2014a;Bordes et al,2014b）学习了问题词的和知识库成分的低维向量表示，并使用向量的和来表示问题和候选答案。然而，简单的相加忽略了词的顺序信息和high-order n-grams.比如，问题who killed A和who A killed在向量相加模型中是一样的。本文使用多列卷积神经网络，能够处理复杂的问题模式。另外，我们的多列网络架构通过学习多列网络可以区分答案类型信息，答案路径和答案上下文信息，而addtion模型把他们混合在一起。

另一种相关的工作是应用深度学习技术来处理问答任务。Grefenstette et al.(2014)提出了一个深度架构来根据标注好的问题的逻辑形式来学习语义解析。Iyyer et al.(2014)提出了一个依存树递归神经网络来处理竞赛游戏，这个游戏要求玩家根据一个给定的段落来回答一个实体。Yu et al.(2014)提出了一个基于卷积神经网络的二元模型来从文本数据中选择答案句子。这个模型学习了问题和答案之间的相似度。Yih et al.(2014)使用卷积神经网络来回答REVERB中的单个关系的问题。然而，本系统工作在关系-实体三元组而不是更加结构化的知识库。

## Setup

给定一个自然语言问题q=w1……wn,我们从FreeBase中检索相关的实体和属性，并把这些作为候选答案Cq.我们的目标是对这些候选答案进行评分并预测答案。比如，问题“when did Avatar release in UK ”的答案是"2009-12-17".应该注意的是，一个问题可能会有多个正确的答案。为了训练模型，我们使用为标注逻辑形式的问答对。下面详细介绍我们的数据集：
_WebQuestions_: 这个数据集(Berant et al,2013)包含3778条训练实例和2032测试实例。我们把训练实例分为80%训练集和20%的开发集。这些问题是通过查询Google Suggenst API来得到的。使用广度优先查询以wh-开头的句子。然后，在Amazon Mechanical Turk来标注答案。所有的答案都可以在FreeBase中找到。
_FreeBase_: Freebase是一个大规模的知识库，由很多facts组成（Bollacker et.al）.这些事实组织成：subject-poperty-object三元组。比如，“Avatar is directed by James Cameron” 用RDF格式表示成 （/m/obth54,film.film.directed_by,/m/03_gd）。
_WikiAnswers_: Fader et al.(2013) 从WikiAnswers抽取了相似的问题，并使用这些问题作为问题释义（paraphrase）。共有350,000个paraphrase clusters，包含大约2 million的问题。他们被用来生成未出现过的词和问题模式。

## Methonds

我们使用多列卷积神经网络（MCCNNs）来学习问题的表示。这个模型共享同样的词嵌入，并且有多列卷积神经网络。在我们的QA任务中列的数目是3列。这些列用来分析问题的不同方面，i.e.,答案路径，答案上下文，和答案类型。通过这些列学习的向量表示记为：f1(q),f2(q),f3(q).我们也学习了Freebase中的候选答案的嵌入。对于每个候选答案我们计算它的向量表示并记为g1(a),g2(a),g3(a).这三个向量与问题理解的三个方面相对应。使用这些向量表示来表示问题和答案，我们可以计算问答对的评分（q,a）。特别的，评分函数S(q,a)定义为：
S(q,a)=f1(q)Tg1(a)+f2(q)Tg2(a)+f3(q)Tg3(a)

其中，fi(q)和gi(a)有相同的维度。

## 4.1候选答案生成

第一步是从Freebase里检索候选答案，问题应该包含可以链接到知识库的实体。我们使用Freebase Search API(Bollacker et al.,2008)来查询问题中的命名实体。如果没有命名实体，名词短语就作为问题实体。我们使用API返回的实体列表中排名最高的一个。这个实体解决方法同样在（Yao and VanDurme,2014）中使用。虽然可以开发更好的方法，但是这并不是本论文的焦点。然后所有与链接的实体相关的的2-hops节点作为候选答案。我们使用q和Cq来表示问题和候选集。

## MCCNNs 问题理解
 MCCNNs使用多个卷积神经网络用共享的输入词嵌入来学习问题的不同方面。对于每列，使用(Collobert et al,2011)提出的网络结构来处理可变长度的问题。
  该模型如图1所示。具体的，对于问题q=w1……wn,lookup层把每个单词转换成向量wj=Wv*u(wj),这里Wv属于dv*V的空间，是词嵌入矩阵，u(wj)是wj的one-hot表示，V是词库大小。词嵌入是参数并且会在训练的过程中更新。
也就是词用的是one-hot表示，然后输入到卷积网络中，得到中间层的输出就是wj.

然后，卷积层在一个滑动的窗口里计算词的表示。对于MCCNNs的第i个列，卷积层为问题q计算n个向量。

最后，一个max-pooling层用来获得问题的固定长度的向量表示。

## 训练

对于问题q的每个正确的答案a，我们随机的从候选答案中采样k个错误的答案a'，并使用它们作为负样本来估计参数。更具体的，hinge损失函数同时考虑（q,a）和（q,a’）：l(q,a,a')取0和（m-S（q,a）+S(q,a')）两者中最大的。

负样本的相似度应该尽量小，正样本与问题的相似度应该尽量大，那么
S(q,a')-S(q,a)应该尽量小，这个差值作为损失函数。

